{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92778525",
      "metadata": {
        "id": "92778525"
      },
      "source": [
        "<font size=\"+3\"><b>Assignment 2: Linear Models and Validation Metrics</b></font>\n",
        "\n",
        "***\n",
        "* **Full Name** = Sudarshan Naicker \n",
        "* **UCID** = 30162797\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce31b39a",
      "metadata": {
        "id": "ce31b39a"
      },
      "source": [
        "<font color='Blue'>In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment.</font>\n",
        "\n",
        "You can use the Table of Content on the left side of this notebook to efficiently navigate within this documents.\n",
        "\n",
        "|                **Question**                | **Point** |\n",
        "|:------------------------------------------:|:---------:|\n",
        "|         **Part 1: Classification**         |           |\n",
        "|          Step 0: Import Libraries          |           |\n",
        "|             Step 1: Data Input             |     1     |\n",
        "|           Step 2: Data Processing          |    1.5    |\n",
        "| Step 3: Implement Machine Learning   Model |           |\n",
        "|           Step 4: Validate Model           |           |\n",
        "|          Step 5: Visualize Results         |     4     |\n",
        "|                  Questions                 |     4     |\n",
        "|             Process Description            |     4     |\n",
        "|           **Part 2: Regression**           |           |\n",
        "|             Step 1: Data Input             |     1     |\n",
        "|           Step 2: Data Processing          |    0.5    |\n",
        "| Step 3: Implement Machine Learning   Model |     1     |\n",
        "|            Step 4: Validate Mode           |     1     |\n",
        "|          Step 5: Visualize Results         |     1     |\n",
        "|                  Questions                 |     2     |\n",
        "|             Process Description            |     4     |\n",
        "|  **Part 3:   Observations/Interpretation** |   **3**   |\n",
        "|           **Part 4: Reflection**           |   **2**   |\n",
        "|                  **Total**                 |   **30**  |\n",
        "|                                            |           |\n",
        "|                  **Bonus**                 |           |\n",
        "|         **Part 5: Bonus Question**         |   **4**   |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c6de86",
      "metadata": {
        "id": "f7c6de86"
      },
      "source": [
        "# **Part 1: Classification (14.5 marks total)**\n",
        "\n",
        "|                **Question**                | **Point** |\n",
        "|:------------------------------------------:|:---------:|\n",
        "|         **Part 1: Classification**         |           |\n",
        "|          Step 0: Import Libraries          |           |\n",
        "|             Step 1: Data Input             |     1     |\n",
        "|           Step 2: Data Processing          |    1.5    |\n",
        "| Step 3: Implement Machine Learning   Model |           |\n",
        "|           Step 4: Validate Model           |           |\n",
        "|          Step 5: Visualize Results         |     4     |\n",
        "|                  Questions                 |     4     |\n",
        "|             Process Description            |     4     |\n",
        "|                  **Total**                 |  **14.5** |\n",
        "\n",
        "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e3c6fc8",
      "metadata": {
        "id": "7e3c6fc8"
      },
      "source": [
        "## **Step 0:** Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "33f86925",
      "metadata": {
        "id": "33f86925"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9d33a8",
      "metadata": {
        "id": "5f9d33a8"
      },
      "source": [
        "## **Step 1:** Data Input (1 mark)\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
        "\n",
        "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
        "\n",
        "Print the size and type of `X` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "33583c67",
      "metadata": {
        "id": "33583c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of X: 262200\n",
            "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
            "Size of y: 4600\n",
            "Type of y: <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Import spam dataset from yellowbrick library\n",
        "from yellowbrick.datasets import load_spam\n",
        "\n",
        "# TO DO: Print size and type of X and y\n",
        "X, y = load_spam()\n",
        "print('Size of X:', X.size)\n",
        "print('Type of X:', type(X))\n",
        "print('Size of y:', y.size)\n",
        "print('Type of y:', type(y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156db208",
      "metadata": {
        "id": "156db208"
      },
      "source": [
        "## **Step 2:** Data Processing (1.5 marks)\n",
        "\n",
        "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4e7204f5",
      "metadata": {
        "id": "4e7204f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values in X:\n",
            " word_freq_make                0\n",
            "word_freq_address             0\n",
            "word_freq_all                 0\n",
            "word_freq_3d                  0\n",
            "word_freq_our                 0\n",
            "word_freq_over                0\n",
            "word_freq_remove              0\n",
            "word_freq_internet            0\n",
            "word_freq_order               0\n",
            "word_freq_mail                0\n",
            "word_freq_receive             0\n",
            "word_freq_will                0\n",
            "word_freq_people              0\n",
            "word_freq_report              0\n",
            "word_freq_addresses           0\n",
            "word_freq_free                0\n",
            "word_freq_business            0\n",
            "word_freq_email               0\n",
            "word_freq_you                 0\n",
            "word_freq_credit              0\n",
            "word_freq_your                0\n",
            "word_freq_font                0\n",
            "word_freq_000                 0\n",
            "word_freq_money               0\n",
            "word_freq_hp                  0\n",
            "word_freq_hpl                 0\n",
            "word_freq_george              0\n",
            "word_freq_650                 0\n",
            "word_freq_lab                 0\n",
            "word_freq_labs                0\n",
            "word_freq_telnet              0\n",
            "word_freq_857                 0\n",
            "word_freq_data                0\n",
            "word_freq_415                 0\n",
            "word_freq_85                  0\n",
            "word_freq_technology          0\n",
            "word_freq_1999                0\n",
            "word_freq_parts               0\n",
            "word_freq_pm                  0\n",
            "word_freq_direct              0\n",
            "word_freq_cs                  0\n",
            "word_freq_meeting             0\n",
            "word_freq_original            0\n",
            "word_freq_project             0\n",
            "word_freq_re                  0\n",
            "word_freq_edu                 0\n",
            "word_freq_table               0\n",
            "word_freq_conference          0\n",
            "char_freq_;                   0\n",
            "char_freq_(                   0\n",
            "char_freq_[                   0\n",
            "char_freq_!                   0\n",
            "char_freq_$                   0\n",
            "char_freq_#                   0\n",
            "capital_run_length_average    0\n",
            "capital_run_length_longest    0\n",
            "capital_run_length_total      0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in y:  0\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Check if there are any missing values and fill them in if necessary\n",
        "\n",
        "missing_values_X = X.isnull().sum()\n",
        "missing_values_y = y.isnull().sum()\n",
        "print(\"Missing values in X:\\n\", missing_values_X)\n",
        "print(\"\\nMissing values in y: \", missing_values_y)\n",
        "\n",
        "#No missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a489285a",
      "metadata": {
        "id": "a489285a"
      },
      "source": [
        "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f9bc4a23",
      "metadata": {
        "id": "f9bc4a23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(230,)\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Create X_small and y_small\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_small, _, y_small, _ = train_test_split(X, y, test_size=0.95, random_state=42)\n",
        "\n",
        "print(y_small.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e6c46f",
      "metadata": {
        "id": "70e6c46f"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model\n",
        "\n",
        "1. Import `LogisticRegression` from sklearn\n",
        "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
        "3. Implement the machine learning model with three different datasets:\n",
        "    - `X` and `y`\n",
        "    - Only first two columns of `X` and `y`\n",
        "    - `X_small` and `y_small`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b89f3d84",
      "metadata": {
        "id": "b89f3d84"
      },
      "source": [
        "## **Step 4:** Validate Model\n",
        "\n",
        "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "352106a3",
      "metadata": {
        "id": "352106a3"
      },
      "source": [
        "## **Step 5:** Visualize Results (4 marks for steps 3-5)\n",
        "\n",
        "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
        "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "be4b5c0a",
      "metadata": {
        "id": "be4b5c0a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Data size</th>\n",
              "      <th>Training accuracy</th>\n",
              "      <th>Validation accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>X and y Dataset</th>\n",
              "      <td>262200.0</td>\n",
              "      <td>0.927989</td>\n",
              "      <td>0.936957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>First two columns</th>\n",
              "      <td>9200.0</td>\n",
              "      <td>0.614946</td>\n",
              "      <td>0.593478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>X_small &amp; y_small Dataset</th>\n",
              "      <td>13110.0</td>\n",
              "      <td>0.961957</td>\n",
              "      <td>0.891304</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                           Data size  Training accuracy  Validation accuracy\n",
              "X and y Dataset             262200.0           0.927989             0.936957\n",
              "First two columns             9200.0           0.614946             0.593478\n",
              "X_small & y_small Dataset    13110.0           0.961957             0.891304"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
        "# Note: for any random state parameters, you can use random_state = 0\n",
        "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Initializing the logistic regression model \n",
        "model = LogisticRegression(max_iter=2000)\n",
        "\n",
        "# Function to train and validate the model\n",
        "def model_trainer(model, X, y):\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "    \n",
        "    # Training the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Calculating the training and validation accuracies\n",
        "    training_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
        "    validation_accuracy = accuracy_score(y_validation, model.predict(X_validation))\n",
        "    \n",
        "    return X.size, training_accuracy, validation_accuracy\n",
        "\n",
        "\n",
        "# Creating the results DataFrame\n",
        "results = pd.DataFrame(columns=['Data size', 'Training accuracy', 'Validation accuracy'])\n",
        "\n",
        "# Implementing the machine learning model with three different datasets\n",
        "datasets = {\n",
        "    'X and y Dataset': (X, y),\n",
        "    'First two columns': (X.iloc[:, :2], y),\n",
        "    'X_small & y_small Dataset': (X_small, y_small)\n",
        "}\n",
        "\n",
        "# training each dataset and storing the scores in results df\n",
        "for name, (data, target) in datasets.items():\n",
        "    results.loc[name] =  model_trainer(model, data, target)\n",
        "\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4427d4f",
      "metadata": {
        "id": "d4427d4f"
      },
      "source": [
        "## **Questions (4 marks)**\n",
        "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
        "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
        "\n",
        "<font color='Green'><b>YOUR ANSWERS HERE</b></font>\n",
        "\n",
        "1) Different sizes of data changes the scores drasticaly:\n",
        "    - X and y Dataset: since we have used all 57 Features with 4600 data for each feature which is a more than enough for training and      \n",
        "      validation. Hence we see a score of 92.7% for training accuracy and 93.6% for  validation accuracy.\n",
        "    - First two columns: we have reduced the dataset to only 2 features hence the accuracies drops significantly, as seen the training accuracy score is only 61.4 % and the Validation accuracy is only 59.3 %  This is because the model has been trained on smaller dataset which is not sufficient for the model to learn the relationships between the spam and non-spam emails.\n",
        "    - small_X and small_y: though we only use 5% for the training dataset we see a high score on both traning and validation of 96% and 89% respectively this shows that a minimum amount of dataset is good enough for the model to learn the complex patterns. However, more the training and testing set available more the information will be received by any machine learning model and better it will perform as seen in results the validation score has been decreased to 89% which is low compared to the full dataset. \n",
        "\n",
        "2) In the case of the email's dataset, \n",
        "  - Flase positive (FP) means, if a model predicts a spam email it is more likely to predict an non-spam email as spam too (Type I error).\n",
        "  - Flase negative (FN) means, if a model predicts a correct emails then it is more likely to predict the spam emails as non-spam (Type II error).\n",
        "\n",
        "  - In this case both FP and FN are bad, but i believe that FP is more worse than FN cause having a spam email classified as non-spam could lead to serious issues like having sensitve information hacked if the user uses the email etc.,"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7559517a",
      "metadata": {
        "id": "7559517a"
      },
      "source": [
        "## **Process Description (4 marks)**\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fe687f",
      "metadata": {
        "id": "59fe687f"
      },
      "source": [
        "<font color='Green'><b>DESCRIBE YOUR PROCESS HERE</b></font>\n",
        "\n",
        "1) I have used the ensf 444 labs, mainly the linear_regression_lab2 was very helful to for the train_test_split() and model_fit(), to train the model. I had to review all 3 labs for the basics like shape/size of datset, accuracy_score() function etc.\n",
        "2) I completed Part 1 in ascending order from step 1-5.\n",
        "4) I had challenges with creating the results dataframe to print the value of the data size training and validation scores as I was calling the model_trainer() funtion each time for the different dataset but after reading the HINT to use 'For loop' I imporvised and created a for loop to call the function this way it is easier to add more datasets in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb4c78a8",
      "metadata": {
        "id": "fb4c78a8"
      },
      "source": [
        "# **Part 2: Regression (10.5 marks total)**\n",
        "\n",
        "| **Question**                               | **Point** |\n",
        "|--------------------------------------------|-----------|\n",
        "| **Part 2: Regression**                     |           |\n",
        "| Step 1: Data Input                         | 1         |\n",
        "| Step 2: Data Processing                    | 0.5       |\n",
        "| Step 3: Implement Machine Learning   Model | 1         |\n",
        "| Step 4: Validate Mode                      | 1         |\n",
        "| Step 5: Visualize Results                  | 1         |\n",
        "| Questions                                  | 2         |\n",
        "| Process Description                        | 4         |\n",
        "| **Total**                                  | **10.5**  |\n",
        "\n",
        "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ba83c5",
      "metadata": {
        "id": "b2ba83c5"
      },
      "source": [
        "## **Step 1:** Data Input (1 mark)\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
        "\n",
        "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
        "\n",
        "Print the size and type of `X` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6ff2e34f",
      "metadata": {
        "id": "6ff2e34f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of X: 8240\n",
            "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
            "Size of y: 1030\n",
            "Type of y: <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Import spam dataset from yellowbrick library\n",
        "from yellowbrick.datasets import load_concrete\n",
        "\n",
        "# TO DO: Print size and type of X and y\n",
        "X2, y2 = load_concrete()\n",
        "print('Size of X:', X2.size)\n",
        "print('Type of X:', type(X2))\n",
        "print('Size of y:', y2.size)\n",
        "print('Type of y:', type(y2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5294cfa",
      "metadata": {
        "id": "c5294cfa"
      },
      "source": [
        "## **Step 2:** Data Processing (0.5 marks)\n",
        "\n",
        "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "693c5fa3",
      "metadata": {
        "id": "693c5fa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values in X:\n",
            " cement    0\n",
            "slag      0\n",
            "ash       0\n",
            "water     0\n",
            "splast    0\n",
            "coarse    0\n",
            "fine      0\n",
            "age       0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in y:  0\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Check if there are any missing values and fill them in if necessary\n",
        "missing_values_X = X2.isnull().sum()\n",
        "missing_values_y = y2.isnull().sum()\n",
        "print(\"Missing values in X:\\n\", missing_values_X)\n",
        "print(\"\\nMissing values in y: \", missing_values_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bc60489",
      "metadata": {
        "id": "1bc60489"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model (1 mark)\n",
        "\n",
        "1. Import `LinearRegression` from sklearn\n",
        "2. Instantiate model `LinearRegression()`.\n",
        "3. Implement the machine learning model with `X` and `y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "suiGuK-W1WnL",
      "metadata": {
        "id": "suiGuK-W1WnL"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b5041945",
      "metadata": {
        "id": "b5041945"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "# Note: for any random state parameters, you can use random_state = 0\n",
        "\n",
        "model2 = LinearRegression()\n",
        "X_train_2, X_validation_2, y_train_2, y_validation_2 = train_test_split(X2, y2, test_size=0.2, random_state=0)\n",
        "model2.fit(X_train_2, y_train_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de28482",
      "metadata": {
        "id": "1de28482"
      },
      "source": [
        "## **Step 4:** Validate Model (1 mark)\n",
        "\n",
        "Calculate the training and validation accuracy using mean squared error and R2 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "970c038b",
      "metadata": {
        "id": "970c038b"
      },
      "outputs": [],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "# Calculate Mean Squared Error (MSE) for both sets\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "training_prediction = model2.predict(X_train_2)\n",
        "validation_prediction = model2.predict(X_validation_2)\n",
        "\n",
        "mse_training = mean_squared_error(y_train_2, training_prediction)\n",
        "mse_validation = mean_squared_error(y_validation_2, validation_prediction)\n",
        "\n",
        "# Calculate R² score for both sets\n",
        "r2_training = r2_score(y_train_2, training_prediction)\n",
        "r2_validation = r2_score(y_validation_2, validation_prediction)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54aa7795",
      "metadata": {
        "id": "54aa7795"
      },
      "source": [
        "## **Step 5:** Visualize Results (1 mark)\n",
        "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
        "2. Add the accuracy results to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "88d223f3",
      "metadata": {
        "id": "88d223f3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training accuracy</th>\n",
              "      <th>Validation accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MSE</th>\n",
              "      <td>110.345501</td>\n",
              "      <td>95.635335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>R2 score</th>\n",
              "      <td>0.609071</td>\n",
              "      <td>0.636898</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Training accuracy Validation accuracy\n",
              "MSE             110.345501           95.635335\n",
              "R2 score          0.609071            0.636898"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "\n",
        "results2 = pd.DataFrame(index=['MSE', 'R2 score'], columns=['Training accuracy', 'Validation accuracy'])\n",
        "\n",
        "\n",
        "results2.loc['MSE', 'Training accuracy'] = mse_training\n",
        "results2.loc['MSE', 'Validation accuracy'] = mse_validation\n",
        "results2.loc['R2 score', 'Training accuracy'] = r2_training\n",
        "results2.loc['R2 score', 'Validation accuracy'] = r2_validation\n",
        "\n",
        "results2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a42bda",
      "metadata": {
        "id": "70a42bda"
      },
      "source": [
        "## **Questions (2 marks)**\n",
        "1. Did using a linear model produce good results for this dataset? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9a6f118",
      "metadata": {},
      "source": [
        "We can see the R2 scores are around 60% which indicates that the linear model has a moderate fit of the dataset. This means that the linear model is not able to handle all the datapoints i.e., has captured some of the variance in the target variable, but not all. If we were aiming for a high precision then the linear model's performance is not optimal."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca0ff2f",
      "metadata": {
        "id": "2ca0ff2f"
      },
      "source": [
        "## **Process Description (4 marks)**\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfdb0880",
      "metadata": {
        "id": "dfdb0880"
      },
      "source": [
        "<font color='Green'><b>Explain YOUR PROCESS here:</b></font>\n",
        "\n",
        "\n",
        "1) I took the help of the linear_regression_lab_2 as it had a linear_regression() example with code for calculating MSE.\n",
        "2) I completed in an ascending order from steps 1-5.\n",
        "3) No, I didn't need to use generative AI\n",
        "4) No challenges as this was very simillar to part 1: Classifications hence when i started this section it was much easier as i was already 'trained' (see what I did there?)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e72ac3eb",
      "metadata": {
        "id": "e72ac3eb"
      },
      "source": [
        "# **Part 3: Observations/Interpretation (3 marks)**\n",
        "\n",
        "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
        "\n",
        "\n",
        "<font color='Green'><b>ADD YOUR FINDINGS HERE</b></font>\n",
        "\n",
        "Part 1:\n",
        "As per the results we can conclude that higher the dataset size, better with the acuraccy scores, as you can see the validation scores for only first 2 columns, whose size was 9200, was the least i.e., 61% only. When we increased the size of the dataset the validation scores increased, 89% for size 13,110 and 93.7% for the 262,200 dataset size. also an interesting point to note here is that even though we are using only 5% of the dataset (13110), the training score is 96% which is higher than the training score for the full dataset (262,220) which is 92% only, this could be due to many reason, but what best reasonates with me is that the 5% dataset is large enough for the model to learn the patterns but small enough to memorize the relationships and have a perfect score. \n",
        "\n",
        "Part 2:\n",
        "we can see that the R2 scores doesnt drastically change for the training and validation test this shows that the model is able to accomdate a moderate amount of variance even after being trained this might be because there are some datapoints that are outside the linear fit.\n",
        "\n",
        "The MSE scores do change from 110 to 95, this means that the model is able to make better predictions for validation set than the training set since lower the MSE score the closer is the fit of the model to the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40b84eed",
      "metadata": {
        "id": "40b84eed"
      },
      "source": [
        "# **Part 4: Reflection (2 marks)**\n",
        "Include a sentence or two about:\n",
        "- what you liked or disliked,\n",
        "- found interesting, confusing, challangeing, motivating\n",
        "while working on this assignment.\n",
        "\n",
        "\n",
        "<font color='Green'><b>ADD YOUR THOUGHTS HERE</b></font>\n",
        "\n",
        "I enjoyed learning how to train the model on the dataset and calculating the R2, MSE and accuracy scores. I didnt find this assignment very challenging but I had to research a bit to understand how training and validation works, the labs and the lecture notes helped me understand the code and definition respectively. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db951b3a",
      "metadata": {
        "id": "db951b3a"
      },
      "source": [
        "# **Part 5: Bonus Question (4 marks)**\n",
        "\n",
        "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
        "\n",
        "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "47623d44",
      "metadata": {
        "id": "47623d44"
      },
      "outputs": [],
      "source": [
        "# TO DO: ADD YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b606236",
      "metadata": {
        "id": "1b606236"
      },
      "source": [
        "<font color='Green'><b>ADD YOUR ANSWER HERE</b></font>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "0.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
