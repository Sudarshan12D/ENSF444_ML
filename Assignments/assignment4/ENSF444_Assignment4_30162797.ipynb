{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92778525",
      "metadata": {
        "id": "92778525"
      },
      "source": [
        "<font size=\"+3\"><b>Assignment 4: Pipelines and Hyperparameter Tuning</b></font>\n",
        "\n",
        "***\n",
        "* **Full Name** = Sudarshan Naicker\n",
        "* **UCID** = 30162797\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce31b39a",
      "metadata": {
        "id": "ce31b39a"
      },
      "source": [
        "<font color='Blue'>\n",
        "In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models, and evaluate the results. More details for each step can be found below. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T0uItvnoRoUB",
      "metadata": {
        "id": "T0uItvnoRoUB"
      },
      "source": [
        "<font color='Red'>\n",
        "For this assignment, in addition to your .ipynb file, please also attach a PDF file. To generate this PDF file, you can use the print function (located under the \"File\" within Jupyter Notebook). Name this file ENGG444_Assignment##__yourUCID.pdf (this name is similar to your main .ipynb file). We will evaluate your assignment based on the two files and you need to provide both.\n",
        "</font>\n",
        "\n",
        "\n",
        "|         **Question**         | **Point(s)** |\n",
        "|:----------------------------:|:------------:|\n",
        "|  **1. Preprocessing Tasks**  |              |\n",
        "|              1.1             |       2      |\n",
        "|              1.2             |       2      |\n",
        "|              1.3             |       4      |\n",
        "| **2. Pipeline and Modeling** |              |\n",
        "|              2.1             |       3      |\n",
        "|              2.2             |       6      |\n",
        "|              2.3             |       5      |\n",
        "|              2.4             |       3      |\n",
        "|     **3. Bonus Question**    |     **2**    |\n",
        "|           **Total**          |    **25**    |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OpeMjIV9VLgM",
      "metadata": {
        "id": "OpeMjIV9VLgM"
      },
      "source": [
        "## **0. Dataset**\n",
        "\n",
        "This data is a subset of the **Heart Disease Dataset**, which contains information about patients with possible coronary artery disease. The data has **14 attributes** and **294 instances**. The attributes include demographic, clinical, and laboratory features, such as age, sex, chest pain type, blood pressure, cholesterol, and electrocardiogram results. The last attribute is the **diagnosis of heart disease**, which is a categorical variable with values from 0 (no presence) to 4 (high presence). The data can be used for **classification** tasks, such as predicting the presence or absence of heart disease based on the other attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "YiaUdCQYVWj-",
      "metadata": {
        "id": "YiaUdCQYVWj-"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>120.0</td>\n",
              "      <td>243.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>140.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>170.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>160.0</td>\n",
              "      <td>331.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>130.0</td>\n",
              "      <td>294.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>155.0</td>\n",
              "      <td>342.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>180.0</td>\n",
              "      <td>393.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
              "0     28    1   2     130.0  132.0  0.0      2.0    185.0    0.0      0.0   \n",
              "1     29    1   2     120.0  243.0  0.0      0.0    160.0    0.0      0.0   \n",
              "2     29    1   2     140.0    NaN  0.0      0.0    170.0    0.0      0.0   \n",
              "3     30    0   1     170.0  237.0  0.0      1.0    170.0    0.0      0.0   \n",
              "4     31    0   2     100.0  219.0  0.0      1.0    150.0    0.0      0.0   \n",
              "..   ...  ...  ..       ...    ...  ...      ...      ...    ...      ...   \n",
              "289   52    1   4     160.0  331.0  0.0      0.0     94.0    1.0      2.5   \n",
              "290   54    0   3     130.0  294.0  0.0      1.0    100.0    1.0      0.0   \n",
              "291   56    1   4     155.0  342.0  1.0      0.0    150.0    1.0      3.0   \n",
              "292   58    0   2     180.0  393.0  0.0      0.0    110.0    1.0      1.0   \n",
              "293   65    1   4     130.0  275.0  0.0      1.0    115.0    1.0      1.0   \n",
              "\n",
              "     slope  ca  thal  num  \n",
              "0      NaN NaN   NaN    0  \n",
              "1      NaN NaN   NaN    0  \n",
              "2      NaN NaN   NaN    0  \n",
              "3      NaN NaN   6.0    0  \n",
              "4      NaN NaN   NaN    0  \n",
              "..     ...  ..   ...  ...  \n",
              "289    NaN NaN   NaN    1  \n",
              "290    2.0 NaN   NaN    1  \n",
              "291    2.0 NaN   NaN    1  \n",
              "292    2.0 NaN   7.0    1  \n",
              "293    2.0 NaN   NaN    1  \n",
              "\n",
              "[294 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the data source link\n",
        "_link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data'\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame, considering '?' as missing values\n",
        "df = pd.read_csv(_link, na_values='?',\n",
        "                 names=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs',\n",
        "                        'restecg', 'thalach', 'exang', 'oldpeak', 'slope',\n",
        "                        'ca', 'thal', 'num'])\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mlcrJpGLWBOH",
      "metadata": {
        "id": "mlcrJpGLWBOH"
      },
      "source": [
        "# **1. Preprocessing Tasks**\n",
        "\n",
        "- **1.1** Find out which columns have more than 60% of their values missing and drop them from the data frame. Explain why this is a reasonable way to handle these columns. **(2 Points)**\n",
        "\n",
        "- **1.2** For the remaining columns that have some missing values, choose an appropriate imputation method to fill them in. You can use the `SimpleImputer` class from `sklearn.impute` or any other method you prefer. Explain why you chose this method and how it affects the data. **(2 Points)**\n",
        "\n",
        "- **1.3** Assign the `num` column to the variable `y` and the rest of the columns to the variable `X`. The `num` column indicates the presence or absence of heart disease based on the angiographic disease status of the patients. Create a `ColumnTransformer` object that applies different preprocessing steps to different subsets of features. Use `StandardScaler` for the numerical features, `OneHotEncoder` for the categorical features, and `passthrough` for the binary features. List the names of the features that belong to each group and explain why they need different transformations. You will use this `ColumnTransformer` in a pipeline in the next question. **(4 Points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yyRJQ25hXHNF",
      "metadata": {
        "id": "yyRJQ25hXHNF"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.1** .....................\n",
        "\n",
        "\n",
        "The columns, slope, ca and thal have a large number of missing values, this means that these features wont be able to give much \n",
        "        information to the model to predict the  target variable. This could also lead to bias as some values are present and most are missing,\n",
        "        hence it is better to remove the entire column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "NzUkBHBfYBzF",
      "metadata": {
        "id": "NzUkBHBfYBzF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "age          0.000000\n",
            "sex          0.000000\n",
            "cp           0.000000\n",
            "trestbps     0.340136\n",
            "chol         7.823129\n",
            "fbs          2.721088\n",
            "restecg      0.340136\n",
            "thalach      0.340136\n",
            "exang        0.340136\n",
            "oldpeak      0.000000\n",
            "slope       64.625850\n",
            "ca          98.979592\n",
            "thal        90.476190\n",
            "num          0.000000\n",
            "dtype: float64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>120.0</td>\n",
              "      <td>243.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>140.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>170.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>160.0</td>\n",
              "      <td>331.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>130.0</td>\n",
              "      <td>294.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>155.0</td>\n",
              "      <td>342.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>180.0</td>\n",
              "      <td>393.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  num\n",
              "0     28    1   2     130.0  132.0  0.0      2.0    185.0    0.0      0.0    0\n",
              "1     29    1   2     120.0  243.0  0.0      0.0    160.0    0.0      0.0    0\n",
              "2     29    1   2     140.0    NaN  0.0      0.0    170.0    0.0      0.0    0\n",
              "3     30    0   1     170.0  237.0  0.0      1.0    170.0    0.0      0.0    0\n",
              "4     31    0   2     100.0  219.0  0.0      1.0    150.0    0.0      0.0    0\n",
              "..   ...  ...  ..       ...    ...  ...      ...      ...    ...      ...  ...\n",
              "289   52    1   4     160.0  331.0  0.0      0.0     94.0    1.0      2.5    1\n",
              "290   54    0   3     130.0  294.0  0.0      1.0    100.0    1.0      0.0    1\n",
              "291   56    1   4     155.0  342.0  1.0      0.0    150.0    1.0      3.0    1\n",
              "292   58    0   2     180.0  393.0  0.0      0.0    110.0    1.0      1.0    1\n",
              "293   65    1   4     130.0  275.0  0.0      1.0    115.0    1.0      1.0    1\n",
              "\n",
              "[294 rows x 11 columns]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1.1\n",
        "# Add necessary code here.\n",
        "\n",
        "sixty = df.isnull().mean() * 100 # to check the percentage of missing values in each column\n",
        "print(sixty)\n",
        "df_removed = df.drop(columns=['slope', 'ca', 'thal']) #  Droping these columns with > 60% missing values\n",
        "df_removed\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xkk6IDQRXgJM",
      "metadata": {
        "id": "xkk6IDQRXgJM"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.2** .....................\n",
        "\n",
        "\n",
        "I have chosen the mean strategy since all the values are numerical and maintains the variance of the dataset. It works by adding all \n",
        "the non-missing values in a column then divide the sum by the total number of non-missing values for that columns and replaces \n",
        "all the missing values in the column with the mean value, this is repeated for every columns with missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "t7Hw48YkZcCb",
      "metadata": {
        "id": "t7Hw48YkZcCb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "age          0\n",
            "sex          0\n",
            "cp           0\n",
            "trestbps     1\n",
            "chol        23\n",
            "fbs          8\n",
            "restecg      1\n",
            "thalach      1\n",
            "exang        1\n",
            "oldpeak      0\n",
            "num          0\n",
            "dtype: int64\n",
            "\n",
            "age         0\n",
            "sex         0\n",
            "cp          0\n",
            "trestbps    0\n",
            "chol        0\n",
            "fbs         0\n",
            "restecg     0\n",
            "thalach     0\n",
            "exang       0\n",
            "oldpeak     0\n",
            "num         0\n",
            "dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>132.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>243.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>250.848708</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>237.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>219.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>52.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>331.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>54.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>294.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>56.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>342.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>58.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>180.0</td>\n",
              "      <td>393.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>65.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>275.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      age  sex   cp  trestbps        chol  fbs  restecg  thalach  exang  \\\n",
              "0    28.0  1.0  2.0     130.0  132.000000  0.0      2.0    185.0    0.0   \n",
              "1    29.0  1.0  2.0     120.0  243.000000  0.0      0.0    160.0    0.0   \n",
              "2    29.0  1.0  2.0     140.0  250.848708  0.0      0.0    170.0    0.0   \n",
              "3    30.0  0.0  1.0     170.0  237.000000  0.0      1.0    170.0    0.0   \n",
              "4    31.0  0.0  2.0     100.0  219.000000  0.0      1.0    150.0    0.0   \n",
              "..    ...  ...  ...       ...         ...  ...      ...      ...    ...   \n",
              "289  52.0  1.0  4.0     160.0  331.000000  0.0      0.0     94.0    1.0   \n",
              "290  54.0  0.0  3.0     130.0  294.000000  0.0      1.0    100.0    1.0   \n",
              "291  56.0  1.0  4.0     155.0  342.000000  1.0      0.0    150.0    1.0   \n",
              "292  58.0  0.0  2.0     180.0  393.000000  0.0      0.0    110.0    1.0   \n",
              "293  65.0  1.0  4.0     130.0  275.000000  0.0      1.0    115.0    1.0   \n",
              "\n",
              "     oldpeak  num  \n",
              "0        0.0  0.0  \n",
              "1        0.0  0.0  \n",
              "2        0.0  0.0  \n",
              "3        0.0  0.0  \n",
              "4        0.0  0.0  \n",
              "..       ...  ...  \n",
              "289      2.5  1.0  \n",
              "290      0.0  1.0  \n",
              "291      3.0  1.0  \n",
              "292      1.0  1.0  \n",
              "293      1.0  1.0  \n",
              "\n",
              "[294 rows x 11 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1.2\n",
        "# Add necessary code here.\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "df_changed = pd.DataFrame(imputer.fit_transform(df_removed), columns=df_removed.columns)\n",
        "\n",
        "\n",
        "print(f'{df_removed.isnull().sum()}\\n') # Check the missing values before simple imputer\n",
        "print(df_changed.isnull().sum()) # Check the missing values after simple imputer\n",
        "\n",
        "df_changed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TS8GSVmmXoOg",
      "metadata": {
        "id": "TS8GSVmmXoOg"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.3** .....................\n",
        "\n",
        "Categorical features are cp and restecg we are using oneHotEncoder as it is used to convert categorical variables into numerical values. \n",
        "        Numerical features are age, trestbps, chol, thalach, oldpeak. we use standardScaler as it converts numerical data to have a mean of\n",
        "        zero and a standard deviation of one this will help ml models to better understand the input variables as they are scaled to a standard range.\n",
        "        Binary features are sex, fbs, exangnot. They are not processed as they are simple with only 2 different values, 1 or 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "pCxLaTmQXYC7",
      "metadata": {
        "id": "pCxLaTmQXYC7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numerical features: ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
            "Categorical (non-binary) features: ['cp', 'restecg']\n",
            "Binary features: ['sex', 'fbs', 'exang']\n"
          ]
        }
      ],
      "source": [
        "# 1.3\n",
        "# Add necessary code here.\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "\n",
        "y = df_changed['num']\n",
        "X = df_changed.drop(columns=['num'])\n",
        "\n",
        "categorical_features = ['cp', 'restecg'] \n",
        "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
        "binary_features = ['sex', 'fbs', 'exang'] \n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "\n",
        "print(\"Numerical features:\", numerical_features)\n",
        "print(\"Categorical (non-binary) features:\", categorical_features)\n",
        "print(\"Binary features:\", binary_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a245d00",
      "metadata": {
        "id": "2a245d00"
      },
      "source": [
        "# **2. Pipeline and Modeling**\n",
        "\n",
        "- **2.1** Create **three** `Pipeline` objects that take the column transformer from the previous question as the first step and add one or more models as the subsequent steps. You can use any models from `sklearn` or other libraries that are suitable for binary classification. For each pipeline, explain **why** you selected the model(s) and what are their **strengths and weaknesses** for this data set. **(3 Points)**\n",
        "\n",
        "- **2.2** Use `GridSearchCV` to perform a grid search over the hyperparameters of each pipeline and find the best combination that maximizes the cross-validation score. Report the best parameters and the best score for each pipeline. Then, update the hyperparameters of each pipeline using the best parameters from the grid search. **(6 Points)**\n",
        "\n",
        "- **2.3** Form a stacking classifier that uses the three pipelines from the previous question as the base estimators and a meta-model as the `final_estimator`. You can choose any model for the meta-model that is suitable for binary classification. Explain **why** you chose the meta-model and how it combines the predictions of the base estimators. Then, use `StratifiedKFold` to perform a cross-validation on the stacking classifier and present the accuracy scores and F1 scores for each fold. Report the mean and the standard deviation of each score in the format of `mean ± std`. For example, `0.85 ± 0.05`. Interpret the results and compare them with the baseline scores from the previous assignment. **(5 Points)**\n",
        "\n",
        "- **2.4**: Interpret the final results of the stacking classifier and compare its performance with the individual models. Explain how stacking classifier has improved or deteriorated the prediction accuracy and F1 score, and what are the possible reasons for that. **(3 Points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GSpSIu-BY1Kn",
      "metadata": {
        "id": "GSpSIu-BY1Kn"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.1** .....................\n",
        "\n",
        "### Logistic Regression\n",
        "- **Use Case**: Mainly used to predict a binary output variable from one or more input features.\n",
        "- **Strength**: Linear models are simple, interpretable, and fast to train.\n",
        "- **Weakness**: May not perform well on complex or non-linear data.\n",
        "- **Rationale**: Since we do have 3 binary features, I have included the LR model.\n",
        "\n",
        "### Random Forest\n",
        "- **Use Case**: An ensemble learning method that operates by constructing a collection of decision trees and outputting the result by either averaging (Regression) or majority voting (Classification) of all trees.\n",
        "\n",
        "- **Strength**:\n",
        "  - Capable of handling both linear and non-linear data.\n",
        "  - By averaging the results of each tree, we reduce the amount of overfitting.\n",
        "  \n",
        "- **Weakness**:\n",
        "  - More complex, leading to longer training times.\n",
        "  - Doesn’t tend to perform well on very high dimensional, sparse data, such as text data.\n",
        "\n",
        "- **Rationale**: Since the trees are averaged out and make it perform way better than an individual tree, hence I am choosing RF.\n",
        "\n",
        "### Support Vector Classifier\n",
        "- **Use Case**: Support vectors are the data points that lie closest to the decision boundary. SVC is a machine learning model that can perform linear or non-linear classification, regression, and even detect outliers.\n",
        "\n",
        "- **Strength**:\n",
        "  - Effective in high-dimensional spaces.\n",
        "  - Can use different kernel functions for a better fit.\n",
        "\n",
        "- **Weakness**:\n",
        "  - Complex and requires careful selection of the kernel and regularization parameters.\n",
        "\n",
        "- **Rationale**: Since we can select various parameters for a better fit, I am using the SVC model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "qYMtXgFtOBMT",
      "metadata": {
        "id": "qYMtXgFtOBMT"
      },
      "outputs": [],
      "source": [
        "# 2.1\n",
        "# Add necessary code here.\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "lr_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "rf_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "svc_pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', SVC())\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NPSo4pBVe1GR",
      "metadata": {
        "id": "NPSo4pBVe1GR"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.2** ....................."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "sNXYl9WFe3vA",
      "metadata": {
        "id": "sNXYl9WFe3vA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for LR: {'classifier__C': 1, 'classifier__max_iter': 1000, 'classifier__penalty': 'l1', 'classifier__solver': 'saga'}\n",
            "Best score for LR: 0.8196376388077148\n",
            "F1 Score for LR: 0.7306\n",
            "\n",
            "Best parameters for RF: {'classifier__max_depth': 5, 'classifier__n_estimators': 100}\n",
            "Best score for RF: 0.7784921098772648\n",
            "F1 Score for RF: 0.6780\n",
            "\n",
            "Best parameters for SVC: {'classifier__C': 0.1, 'classifier__kernel': 'linear'}\n",
            "Best score for SVC: 0.819812974868498\n",
            "F1 Score for SVC: 0.6972\n"
          ]
        }
      ],
      "source": [
        "# 2.2\n",
        "# Add necessary code here.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "lr_param_grid = {\n",
        "    'classifier__C': [0.1, 1, 10],\n",
        "    'classifier__penalty': ['l1'], \n",
        "    'classifier__solver': ['liblinear', 'saga'],\n",
        "    'classifier__max_iter': [1000, 5000] \n",
        "}\n",
        "\n",
        "rf_param_grid = {\n",
        "    'classifier__n_estimators': [100, 200, 300],\n",
        "    'classifier__max_depth': [5, 10, 15]\n",
        "}\n",
        "\n",
        "svc_param_grid = {\n",
        "    'classifier__C': [0.1, 1, 10],\n",
        "    'classifier__kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "lr_grid_search = GridSearchCV(lr_pipeline, lr_param_grid, cv=5, scoring='accuracy')\n",
        "rf_grid_search = GridSearchCV(rf_pipeline, rf_param_grid, cv=5, scoring='accuracy')\n",
        "svc_grid_search = GridSearchCV(svc_pipeline, svc_param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "lr_grid_search.fit(X, y)\n",
        "rf_grid_search.fit(X, y)\n",
        "svc_grid_search.fit(X, y)\n",
        "\n",
        "#f1 Score\n",
        "f1_scores_lr = cross_val_score(lr_pipeline, X, y, cv=5, scoring='f1').mean()\n",
        "f1_scores_rf = cross_val_score(rf_pipeline, X, y, cv=5, scoring='f1').mean()\n",
        "f1_scores_svc = cross_val_score(svc_pipeline, X, y, cv=5, scoring='f1').mean()\n",
        "\n",
        "\n",
        "# Reporting F1 scores and accuracy scores\n",
        "print(\"Best parameters for LR:\", lr_grid_search.best_params_)\n",
        "print(\"Best score for LR:\", lr_grid_search.best_score_)\n",
        "print(f\"F1 Score for LR: {f1_scores_lr:.4f}\")\n",
        "\n",
        "print(\"\\nBest parameters for RF:\", rf_grid_search.best_params_)\n",
        "print(\"Best score for RF:\", rf_grid_search.best_score_)\n",
        "print(f\"F1 Score for RF: {f1_scores_rf:.4f}\")\n",
        "\n",
        "print(\"\\nBest parameters for SVC:\", svc_grid_search.best_params_)\n",
        "print(\"Best score for SVC:\", svc_grid_search.best_score_)\n",
        "print(f\"F1 Score for SVC: {f1_scores_svc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ygOeNB-PamnU",
      "metadata": {
        "id": "ygOeNB-PamnU"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.3** .....................\n",
        "\n",
        "\n",
        "The meta-model's role is to combine the predictions of all estimators to make a final prediction.\n",
        "Logistic Regression is simple, interpretable, and efficient in combining predictions from various models without adding computational overhead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "UvhsbjmYP2G_",
      "metadata": {
        "id": "UvhsbjmYP2G_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy Scores for individual folds[0.81355932 0.76271186 0.83050847 0.83050847 0.75862069]\n",
            "Accuracy: 0.80 ± 0.03\n",
            "F1 Score: 0.70 ± 0.10\n"
          ]
        }
      ],
      "source": [
        "# 2.3\n",
        "# Add necessary code here.\n",
        "\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', lr_pipeline),\n",
        "        ('rf', rf_pipeline),\n",
        "        ('svc', svc_pipeline)\n",
        "    ],\n",
        "    final_estimator= LogisticRegression()\n",
        ")\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5)\n",
        "accuracy_scores = cross_val_score(stacking_clf, X, y, cv=cv, scoring='accuracy')\n",
        "f1_scores = cross_val_score(stacking_clf, X, y, cv=cv, scoring='f1')\n",
        "\n",
        "# Calculate mean and standard deviation for both scores\n",
        "accuracy_mean, accuracy_std = np.mean(accuracy_scores), np.std(accuracy_scores)\n",
        "f1_mean, f1_std = np.mean(f1_scores), np.std(f1_scores)\n",
        "\n",
        "#results\n",
        "print(f\"Accuracy Scores for individual folds: {accuracy_scores}\")\n",
        "print(f\"Accuracy: {accuracy_mean:.2f} ± {accuracy_std:.2f}\")\n",
        "print(f\"F1 Score: {f1_mean:.2f} ± {f1_std:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A-TN9hr3b77-",
      "metadata": {
        "id": "A-TN9hr3b77-"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.4** .....................\n",
        "\n",
        "The stacking classifier shows a mean accuracy of 0.80 which is better compared to the accuracy scores of RF is 0.77 but the BEST score for LR and SVC is 0.81 which is greater than 0.8. However that is the best score of all 5 folds when you compare the each of the folds for stacking_clf we can see that 0.8474 was the best score this means that the overall mean of the LR and SVC would be lower. hence the stacking classifier has performed better in terms of accuracy scores than each of the individual models.\n",
        "\n",
        "For the F1 scores, the stacking classifier's mean F1 score of 0.70 which is better compared to the mean f1 scores of RF and SVC which is 0.68 and 0.69 respectively. However LR's f1 score is 0.7306 but is notably higher. The improvement in the F1 score for the stacking classifier suggests it may be more effective at balancing false positives and false negatives than the RF and SVC. LR model perform better as it is a more simple model, though the stacking classifier uses the LR as meta-model it is trained on the predictions made by the base estimators, the lr_pipeline uses raw feature making it less complicated to make prediction.\n",
        "\n",
        "Overall The Stacking Classifier is a better model as the Accuracy scores are higher than their individual models and the F1 score is also mostly better this is beacuse The stacking model benefits from the diversity of the base estimators, each bringing its strengths and reducing the impact of any one model's weaknesses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RPa-v8Xxc7aU",
      "metadata": {
        "id": "RPa-v8Xxc7aU"
      },
      "source": [
        "**Bonus Question**: The stacking classifier has achieved a high accuracy and F1 score, but there may be still room for improvement. Suggest **two** possible ways to improve the modeling using the stacking classifier, and explain **how** and **why** they could improve the performance. **(2 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IrSooo0DfC-V",
      "metadata": {
        "id": "IrSooo0DfC-V"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
